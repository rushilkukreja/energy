{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCIOAuCzG7HQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting K-Fold Cross-Validation for TCN Model ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Epoch 1/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - loss: 0.0156 - val_loss: 0.0017\n",
      "Epoch 2/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 3/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 22ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 4/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 16ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 5/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 17ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 6/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 16ms/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 7/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 16ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 8/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 15ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 9/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 19ms/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 10/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 11/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 17ms/step - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 12/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 17ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 13/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 14/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 16ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m496/496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Epoch 1/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 20ms/step - loss: 0.0188 - val_loss: 0.0020\n",
      "Epoch 2/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 17ms/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 3/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 4/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 5/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 20ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 7/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 9/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 21ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 18ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 11/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 12/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 21ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 21ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 14/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 19ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 15/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 21ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m496/496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Epoch 1/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 19ms/step - loss: 0.0097 - val_loss: 0.0018\n",
      "Epoch 2/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 18ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 3/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 17ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 4/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 5/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 10ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 6/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 7/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 8/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 9/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 10/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 11/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 10ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m496/496\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Epoch 1/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 11ms/step - loss: 0.0094 - val_loss: 0.0020\n",
      "Epoch 2/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 11ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 3/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 4/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 5/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 28ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 6/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 32ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 7/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 8/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 9/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 10/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 11/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 13ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 12/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 13/30\n",
      "\u001b[1m1783/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 14/30\n",
      "\u001b[1m1780/1783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0014"
     ]
    }
   ],
   "source": [
    "# Tampa Energy Demand Forecasting with TCN (Updated)\n",
    "#\n",
    "# This script builds and evaluates a Temporal Convolutional Network (TCN) model\n",
    "# for energy demand forecasting in Tampa.\n",
    "#\n",
    "# Updates from original version:\n",
    "# - Multi-Horizon Forecasting: Predicts energy demand for 1, 6, 12, and 24 hours ahead.\n",
    "# - Additional Metrics: Evaluation includes MAE, MSE, RMSE, R², and MAPE for each forecast horizon.\n",
    "\n",
    "#REMOVED TEMP\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Conv1D, Activation, SpatialDropout1D, Lambda, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- 1. Data Loading and Feature Engineering ---\n",
    "# Load the dataset\n",
    "# Make sure to replace 'Tampa.csv' with the correct path if the file is not in the same directory.\n",
    "try:\n",
    "    df = pd.read_csv('/Users/rushilkukreja/Downloads/mahabir/Tampa.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Tampa.csv' not found. Please ensure the dataset is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# Convert Demand (MW) to numeric (handle both string and numeric formats)\n",
    "if df['Demand (MW)'].dtype == 'object':\n",
    "    df['Demand (MW)'] = df['Demand (MW)'].str.replace(',', '').astype(float)\n",
    "else:\n",
    "    df['Demand (MW)'] = df['Demand (MW)'].astype(float)\n",
    "\n",
    "# Handle missing values\n",
    "df['Pressure'] = df['Pressure'].fillna(df['Pressure'].mean())\n",
    "df['Precipitation'] = df['Precipitation'].fillna(0)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Create time-based features\n",
    "df['Hour'] = pd.to_datetime(df['time']).dt.hour\n",
    "df['DayOfWeek'] = pd.to_datetime(df['time']).dt.dayofweek\n",
    "df['Month'] = pd.to_datetime(df['time']).dt.month\n",
    "df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Create lag features\n",
    "for lag in range(1, 25):\n",
    "    df[f'Lag_{lag}'] = df['Demand (MW)'].shift(lag)\n",
    "\n",
    "# Create rolling window features\n",
    "df['Temp_RollingMean_24'] = df['Temperature'].rolling(window=24).mean()\n",
    "df['Temp_RollingStd_24'] = df['Temperature'].rolling(window=24).std()\n",
    "\n",
    "# Create interaction features\n",
    "df['Temp_Humidity_Interaction'] = df['Temperature'] * df['RelativeHumidity']\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "features = df[['DewPoint', 'RelativeHumidity', 'Precipitation', 'WindSpeed', 'Pressure',\n",
    "               'Hour', 'DayOfWeek', 'Month', 'IsWeekend',\n",
    "               'Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5', 'Lag_6', 'Lag_7', 'Lag_8', 'Lag_9', 'Lag_10',\n",
    "               'Lag_11', 'Lag_12', 'Lag_13', 'Lag_14', 'Lag_15', 'Lag_16', 'Lag_17', 'Lag_18', 'Lag_19',\n",
    "               'Lag_20', 'Lag_21', 'Lag_22', 'Lag_23', 'Lag_24',\n",
    "               'Temp_Humidity_Interaction']].values\n",
    "energy_demand = df['Demand (MW)'].values.reshape(-1, 1)\n",
    "dates = df['time'].values\n",
    "\n",
    "# Scale features and target\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_energy = MinMaxScaler()\n",
    "features_normalized = scaler_features.fit_transform(features)\n",
    "energy_demand_normalized = scaler_energy.fit_transform(energy_demand)\n",
    "\n",
    "# --- 2. Sequence Creation for Time Series Forecasting ---\n",
    "def create_sequences(data, target, dates, seq_length, horizons):\n",
    "    \"\"\"\n",
    "    Creates sequences of data for time series forecasting.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    sequence_dates = []\n",
    "    for i in range(len(data) - seq_length - max(horizons) + 1):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target_seq = [target[i + seq_length + h - 1][0] for h in horizons]\n",
    "        date_seq = dates[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target_seq)\n",
    "        sequence_dates.append(date_seq)\n",
    "    return np.array(sequences), np.array(targets), np.array(sequence_dates)\n",
    "\n",
    "seq_length = 24\n",
    "horizons = [1, 6, 12, 24]\n",
    "X, y, dates_seq = create_sequences(features_normalized, energy_demand_normalized, dates, seq_length, horizons)\n",
    "\n",
    "# --- 3. Model Definition, Training, and Evaluation ---\n",
    "def residual_block(x, dilation_rate, nb_filters, kernel_size, dropout_rate=0.2):\n",
    "    prev_x = x\n",
    "    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(dropout_rate)(x)\n",
    "    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(dropout_rate)(x)\n",
    "\n",
    "    if prev_x.shape[-1] != x.shape[-1]:\n",
    "        prev_x = Conv1D(filters=nb_filters, kernel_size=1, padding='same')(prev_x)\n",
    "\n",
    "    x = Add()([prev_x, x])\n",
    "    return x\n",
    "\n",
    "def create_tcn_model(input_shape, nb_filters, kernel_size, dilations, nb_stacks, num_horizons):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "    for _ in range(nb_stacks):\n",
    "        for dilation_rate in dilations:\n",
    "            x = residual_block(x, dilation_rate, nb_filters, kernel_size)\n",
    "    x = Lambda(lambda tt: tt[:, -1, :])(x)\n",
    "    output_layer = Dense(num_horizons)(x)\n",
    "    model = Model(input_layer, output_layer)\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "all_metrics = {h: {'mae': [], 'mse': [], 'rmse': [], 'r2': [], 'mape': []} for h in horizons}\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_indices = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_indices] - y_pred[non_zero_indices]) / y_true[non_zero_indices])) * 100\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "print(\"--- Starting K-Fold Cross-Validation for TCN Model ---\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n--- Fold {fold+1}/5 ---\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dates_test = dates_seq[test_index]\n",
    "\n",
    "    model = create_tcn_model(\n",
    "        input_shape=(seq_length, X.shape[2]),\n",
    "        nb_filters=64,\n",
    "        kernel_size=2,\n",
    "        dilations=[1, 2, 4, 8, 16, 32],\n",
    "        nb_stacks=1,\n",
    "        num_horizons=len(horizons)\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    predictions_normalized = model.predict(X_test)\n",
    "    predictions = scaler_energy.inverse_transform(predictions_normalized)\n",
    "    y_test_original = scaler_energy.inverse_transform(y_test)\n",
    "\n",
    "    for i, h in enumerate(horizons):\n",
    "        mae = mean_absolute_error(y_test_original[:, i], predictions[:, i])\n",
    "        mse = mean_squared_error(y_test_original[:, i], predictions[:, i])\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test_original[:, i], predictions[:, i])\n",
    "        mape = mean_absolute_percentage_error(y_test_original[:, i], predictions[:, i])\n",
    "\n",
    "        all_metrics[h]['mae'].append(mae)\n",
    "        all_metrics[h]['mse'].append(mse)\n",
    "        all_metrics[h]['rmse'].append(rmse)\n",
    "        all_metrics[h]['r2'].append(r2)\n",
    "        all_metrics[h]['mape'].append(mape)\n",
    "\n",
    "    fold_results = pd.DataFrame({'Date': dates_test})\n",
    "    for i, h in enumerate(horizons):\n",
    "        fold_results[f'Actual_{h}h'] = y_test_original[:, i]\n",
    "        fold_results[f'Predicted_{h}h'] = predictions[:, i]\n",
    "\n",
    "    result_df = pd.concat([result_df, fold_results], ignore_index=True)\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "print(\"\\n--- Average Performance Metrics for TCN Model ---\")\n",
    "for h in horizons:\n",
    "    print(f'\\nHorizon: {h} hours')\n",
    "    for metric_name, values in all_metrics[h].items():\n",
    "        print(f'  Average {metric_name.upper()}: {np.mean(values):.4f}')\n",
    "\n",
    "# Plot results for the 1-hour horizon as an example\n",
    "print(\"\\n--- Plotting Results for 1-Hour Horizon (First 100 Points) ---\")\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(result_df['Date'][:100], result_df['Actual_1h'][:100], label='Actual 1-hour ahead', marker='o', linestyle='-')\n",
    "plt.plot(result_df['Date'][:100], result_df['Predicted_1h'][:100], label='Predicted 1-hour ahead', marker='x', linestyle='--')\n",
    "plt.title('TCN: Predicted vs Actual Energy Demand for Tampa (1-Hour Horizon)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Energy Demand')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCL9fmM1ERS+zcLClWzvNN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
