{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcc9155",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic: -13.876101\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.430\n",
      "\t5%: -2.862\n",
      "\t10%: -2.567\n",
      "ADF Statistic: -35.930386\n",
      "p-value: 0.000000\n",
      "Critical Values:\n",
      "\t1%: -3.430\n",
      "\t5%: -2.862\n",
      "\t10%: -2.567\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,1)[12]             : AIC=inf, Time=9.60 sec\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=122644.753, Time=0.29 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=117680.151, Time=6.01 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=10.12 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=122634.585, Time=0.29 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12]             : AIC=117473.955, Time=21.91 sec\n",
      " ARIMA(1,1,0)(2,1,1)[12]             : AIC=inf, Time=64.58 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=inf, Time=20.45 sec\n",
      " ARIMA(0,1,0)(2,1,0)[12]             : AIC=117544.169, Time=4.58 sec\n",
      " ARIMA(2,1,0)(2,1,0)[12]             : AIC=117394.604, Time=15.71 sec\n",
      " ARIMA(2,1,0)(1,1,0)[12]             : AIC=117567.586, Time=8.71 sec\n",
      " ARIMA(2,1,0)(2,1,1)[12]             : AIC=inf, Time=70.59 sec\n",
      " ARIMA(2,1,0)(1,1,1)[12]             : AIC=inf, Time=16.24 sec\n",
      " ARIMA(3,1,0)(2,1,0)[12]             : AIC=117351.171, Time=22.80 sec\n",
      " ARIMA(3,1,0)(1,1,0)[12]             : AIC=117508.305, Time=8.66 sec\n",
      " ARIMA(3,1,0)(2,1,1)[12]             : AIC=inf, Time=97.35 sec\n",
      " ARIMA(3,1,0)(1,1,1)[12]             : AIC=inf, Time=30.69 sec\n",
      " ARIMA(4,1,0)(2,1,0)[12]             : AIC=117321.228, Time=32.63 sec\n",
      " ARIMA(4,1,0)(1,1,0)[12]             : AIC=117468.334, Time=15.86 sec\n",
      " ARIMA(4,1,0)(2,1,1)[12]             : AIC=inf, Time=118.79 sec\n",
      " ARIMA(4,1,0)(1,1,1)[12]             : AIC=inf, Time=29.24 sec\n",
      " ARIMA(5,1,0)(2,1,0)[12]             : AIC=117231.687, Time=25.95 sec\n",
      " ARIMA(5,1,0)(1,1,0)[12]             : AIC=117371.079, Time=24.88 sec\n",
      " ARIMA(5,1,0)(2,1,1)[12]             : AIC=inf, Time=123.66 sec\n",
      " ARIMA(5,1,0)(1,1,1)[12]             : AIC=inf, Time=45.33 sec\n",
      " ARIMA(5,1,1)(2,1,0)[12]             : AIC=inf, Time=100.20 sec\n",
      " ARIMA(4,1,1)(2,1,0)[12]             : AIC=inf, Time=89.96 sec\n",
      " ARIMA(5,1,0)(2,1,0)[12] intercept   : AIC=117233.687, Time=118.64 sec\n",
      "\n",
      "Best model:  ARIMA(5,1,0)(2,1,0)[12]          \n",
      "Total fit time: 1133.882 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values found in forecast or test data.\n",
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,1,1)[12]             : AIC=inf, Time=16.19 sec\n",
      " ARIMA(0,1,0)(0,1,0)[12]             : AIC=316711.403, Time=0.45 sec\n",
      " ARIMA(1,1,0)(1,1,0)[12]             : AIC=302520.861, Time=29.62 sec\n",
      " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=49.05 sec\n",
      " ARIMA(1,1,0)(0,1,0)[12]             : AIC=307392.981, Time=0.94 sec\n",
      " ARIMA(1,1,0)(2,1,0)[12]             : AIC=300663.717, Time=105.32 sec\n",
      " ARIMA(1,1,0)(2,1,1)[12]             : AIC=inf, Time=164.48 sec\n",
      " ARIMA(1,1,0)(1,1,1)[12]             : AIC=inf, Time=45.47 sec\n",
      " ARIMA(0,1,0)(2,1,0)[12]             : AIC=310040.475, Time=19.76 sec\n",
      " ARIMA(2,1,0)(2,1,0)[12]             : AIC=296416.275, Time=111.82 sec\n",
      " ARIMA(2,1,0)(1,1,0)[12]             : AIC=298153.828, Time=39.00 sec\n",
      " ARIMA(2,1,0)(2,1,1)[12]             : AIC=inf, Time=172.95 sec\n",
      " ARIMA(2,1,0)(1,1,1)[12]             : AIC=inf, Time=61.47 sec\n",
      " ARIMA(3,1,0)(2,1,0)[12]             : AIC=294223.080, Time=109.17 sec\n",
      " ARIMA(3,1,0)(1,1,0)[12]             : AIC=295785.848, Time=45.73 sec\n",
      " ARIMA(3,1,0)(2,1,1)[12]             : AIC=inf, Time=168.38 sec\n",
      " ARIMA(3,1,0)(1,1,1)[12]             : AIC=inf, Time=79.04 sec\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Conv1D, Activation, SpatialDropout1D, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "df = pd.read_excel('Philadelphia.xlsx')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "df['Hour'] = pd.to_datetime(df['Time']).dt.hour\n",
    "df['DayOfWeek'] = pd.to_datetime(df['Time']).dt.dayofweek\n",
    "df['Month'] = pd.to_datetime(df['Time']).dt.month\n",
    "df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "\n",
    "for lag in range(1, 25):\n",
    "    df[f'Lag_{lag}'] = df['EnergyDemand'].shift(lag)\n",
    "\n",
    "\n",
    "df['Temp_RollingMean_24'] = df['Temperature'].rolling(window=24).mean()\n",
    "df['Temp_RollingStd_24'] = df['Temperature'].rolling(window=24).std()\n",
    "\n",
    "\n",
    "df['Temp_Humidity_Interaction'] = df['Temperature'] * df['RelativeHumidity']\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "features = df[['Temperature', 'DewPoint', 'RelativeHumidity', 'Precipitation', 'WindSpeed', 'Pressure',\n",
    "               'Hour', 'DayOfWeek', 'Month', 'IsWeekend',\n",
    "               'Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5', 'Lag_6', 'Lag_7', 'Lag_8', 'Lag_9', 'Lag_10',\n",
    "               'Lag_11', 'Lag_12', 'Lag_13', 'Lag_14', 'Lag_15', 'Lag_16', 'Lag_17', 'Lag_18', 'Lag_19',\n",
    "               'Lag_20', 'Lag_21', 'Lag_22', 'Lag_23', 'Lag_24',\n",
    "               'Temp_RollingMean_24', 'Temp_RollingStd_24',\n",
    "               'Temp_Humidity_Interaction']].values\n",
    "energy_demand = df['EnergyDemand'].values.reshape(-1, 1)\n",
    "dates = df['Time'].values\n",
    "\n",
    "\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_energy = MinMaxScaler()\n",
    "features_normalized = scaler_features.fit_transform(features)\n",
    "energy_demand_normalized = scaler_energy.fit_transform(energy_demand)\n",
    "\n",
    "def create_sequences(data, target, dates, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    sequence_dates = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target_seq = target[i+seq_length]\n",
    "        date_seq = dates[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target_seq)\n",
    "        sequence_dates.append(date_seq)\n",
    "    return np.array(sequences), np.array(targets), np.array(sequence_dates)\n",
    "\n",
    "seq_length = 24\n",
    "X, y, dates_seq = create_sequences(features_normalized, energy_demand_normalized, dates, seq_length)\n",
    "\n",
    "def residual_block(x, dilation_rate, nb_filters, kernel_size, dropout_rate=0.2):\n",
    "    prev_x = x\n",
    "    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(dropout_rate)(x)\n",
    "    x = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = SpatialDropout1D(dropout_rate)(x)\n",
    "\n",
    "\n",
    "    if prev_x.shape[-1] != x.shape[-1]:\n",
    "        prev_x = Conv1D(filters=nb_filters, kernel_size=1, padding='same')(prev_x)\n",
    "\n",
    "    x = tf.keras.layers.add([prev_x, x])\n",
    "    return x\n",
    "\n",
    "def create_model(input_shape, nb_filters, kernel_size, dilations, nb_stacks):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "    for _ in range(nb_stacks):\n",
    "        for dilation_rate in dilations:\n",
    "            x = residual_block(x, dilation_rate, nb_filters, kernel_size)\n",
    "    x = Lambda(lambda tt: tt[:, -1, :])(x)\n",
    "    output_layer = Dense(1)(x)\n",
    "    model = tf.keras.models.Model(input_layer, output_layer)\n",
    "    return model\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_indices = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_indices] - y_pred[non_zero_indices]) / y_true[non_zero_indices])) * 100\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    dates_test = dates_seq[test_index]\n",
    "\n",
    "    model = create_model(\n",
    "        input_shape=(seq_length, X.shape[2]),\n",
    "        nb_filters=64,\n",
    "        kernel_size=2,\n",
    "        dilations=[1, 2, 4, 8, 16, 32],\n",
    "        nb_stacks=1\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "\n",
    "    predictions_normalized = model.predict(X_test)\n",
    "    predictions = scaler_energy.inverse_transform(predictions_normalized)\n",
    "    y_test_original = scaler_energy.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(y_test_original, predictions)\n",
    "    mae = mean_absolute_error(y_test_original, predictions)\n",
    "    mape = mean_absolute_percentage_error(y_test_original, predictions)\n",
    "\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)\n",
    "\n",
    "    fold_results = pd.DataFrame({\n",
    "        'Date': dates_test,\n",
    "        'Actual': y_test_original.flatten(),\n",
    "        'Predicted': predictions.flatten()\n",
    "    })\n",
    "    result_df = pd.concat([result_df, fold_results], ignore_index=True)\n",
    "\n",
    "\n",
    "average_mse = np.mean(mse_scores)\n",
    "average_mae = np.mean(mae_scores)\n",
    "average_mape = np.mean(mape_scores)\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")\n",
    "print(f\"Average Mean Absolute Error: {average_mae}\")\n",
    "print(f\"Average Mean Absolute Percentage Error: {average_mape}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(result_df['Actual'][:100], label='Actual')\n",
    "plt.plot(result_df['Predicted'][:100], label='Predicted')\n",
    "plt.title('Predicted vs Actual Energy Demand')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Energy Demand')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec122ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
